{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82984cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungwoo/anaconda3/envs/sur/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Fetching 2 files: 100%|██████████| 2/2 [01:32<00:00, 46.27s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([1, 8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Blip2Processor\n",
    "from src.models.surroundblip import SurroundBlip\n",
    "\n",
    "# 1) 환경 설정\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "device = \"mps\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = SurroundBlip.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 2) 더미 이미지 패치 생성 (B=1, P=8, C=3, H=W=224)\n",
    "B, P, C, H, W = 1, 8, 3, 224, 224\n",
    "dummy_pixel_values = torch.randn(B, P, C, H, W, device=device)\n",
    "\n",
    "# 3) 더미 텍스트 입력\n",
    "prompt = \"Question: 이 장면에 무엇이 있나요? Answer:\"\n",
    "text_inputs = processor(\n",
    "    text=prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=32,\n",
    ")\n",
    "input_ids = text_inputs[\"input_ids\"].to(device)\n",
    "attention_mask = text_inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# 4) forward 호출\n",
    "#    return_dict=True 로 하면 namedtuple 형태가 아닌 ModelOutput 형태로 결과를 돌려줍니다.\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        pixel_values=dummy_pixel_values,      # (1, 8, 3, 224, 224)\n",
    "        input_ids=input_ids,                  # (1, 32)\n",
    "        attention_mask=attention_mask,        # (1, 32)\n",
    "        interpolate_pos_encoding=True,\n",
    "        use_cache=False,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "# 5) 출력 정보 확인\n",
    "#    outputs는 Blip2ForConditionalGenerationModelOutput 타입\n",
    "print(\"loss: \", outputs.loss)  # None (labels를 주지 않았으므로)\n",
    "print(\"logits.shape:\", outputs.logits.shape)\n",
    "#   → (batch_size, seq_len, vocab_size), e.g. torch.Size([1, 32+Q, 250112])\n",
    "\n",
    "# 비전 인코더 출력\n",
    "vision_out = outputs.vision_outputs\n",
    "print(\"vision last_hidden_state.shape:\", vision_out.last_hidden_state.shape)\n",
    "#   → (B*P, S_img, D) or (B, P, S_img, D) flatten 전, 예: torch.Size([8, 197, 768])\n",
    "\n",
    "# Q-Former 출력\n",
    "qformer_out = outputs.qformer_outputs\n",
    "print(\"qformer last_hidden_state.shape:\", qformer_out.last_hidden_state.shape)\n",
    "#   → (B, Q, D), 예: torch.Size([1, 32, 768])\n",
    "\n",
    "# 언어 모델 로지츠\n",
    "print(\"language_model_outputs[0].shape:\", outputs.language_model_outputs[0].shape)\n",
    "#   → (B, seq_len_out, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
