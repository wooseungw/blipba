import asyncio
import aiohttp
import pandas as pd
import os
import argparse
from pathlib import Path
from tqdm.asyncio import tqdm

class RateLimiter:
    """
    ì´ˆë‹¹ ìš”ì²­ ìˆ˜(requests_per_second)ë¥¼ ì œí•œí•˜ëŠ” ê°„ë‹¨í•œ ë ˆì´íŠ¸ ë¦¬ë¯¸í„°
    """
    def __init__(self, requests_per_second: float):
        self._interval = 1.0 / requests_per_second
        self._lock = asyncio.Lock()
        self._last_call = None

    async def wait(self):
        async with self._lock:
            now = asyncio.get_event_loop().time()
            if self._last_call is not None:
                elapsed = now - self._last_call
                wait_time = self._interval - elapsed
                if wait_time > 0:
                    await asyncio.sleep(wait_time)
            self._last_call = asyncio.get_event_loop().time()

async def download_image(session, url, cache_path, semaphore, rate_limiter):
    """ë¹„ë™ê¸° ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ. ì„±ê³µ ì‹œ ë¡œì»¬ ê²½ë¡œ, ì‹¤íŒ¨ ì‹œ None ë°˜í™˜."""
    async with semaphore:
        if os.path.exists(cache_path):
            return cache_path
        await rate_limiter.wait()
        try:
            async with session.get(url) as resp:
                if resp.status == 429:
                    retry_after = int(resp.headers.get('Retry-After', 60))
                    await asyncio.sleep(retry_after)
                    return await download_image(session, url, cache_path, semaphore, rate_limiter)
                if resp.status == 200:
                    data = await resp.read()
                    with open(cache_path, 'wb') as f:
                        f.write(data)
                    return cache_path
        except Exception as e:
            print(f"[ERROR] {url} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

async def batch_download_unique(urls, cache_dir, max_concurrent, rps):
    """
    ì¤‘ë³µ URL ì œê±° í›„ í•œ ë²ˆì”©ë§Œ ë‹¤ìš´ë¡œë“œ,
    ë°˜í™˜ê°’: { url: local_path or None, ... }
    """
    os.makedirs(cache_dir, exist_ok=True)
    sem = asyncio.Semaphore(max_concurrent)
    limiter = RateLimiter(rps)
    async with aiohttp.ClientSession() as session:
        tasks = []
        for url in urls:
            fname = os.path.basename(url)
            path = os.path.join(cache_dir, fname)
            tasks.append(download_image(session, url, path, sem, limiter))
        results = await tqdm.gather(*tasks, desc="Downloading unique images")
    return dict(zip(urls, results))

def process_split(
    input_csv: str,
    output_csv: str,
    image_dir: str,
    url_col: str,
    max_concurrent: int,
    rps: float,
    out_dir: str,
    split: str
):
    # 1) CSV ë¡œë“œ
    df = pd.read_csv(input_csv)

    # 2) ìœ ë‹ˆí¬ URL ë‹¤ìš´ë¡œë“œ
    unique_urls = df[url_col].unique().tolist()
    url2path = asyncio.run(batch_download_unique(
        unique_urls,
        cache_dir=image_dir,
        max_concurrent=max_concurrent,
        rps=rps
    ))

    # 3) ë§¤í•‘ & í•„í„°ë§
    df['local_path'] = df[url_col].map(url2path)
    df = df[df['local_path'].notna()].copy()

    # 4) URL ì»¬ëŸ¼ì„ "data/{split}/images/{filename}" ë¡œ ê³ ì •
    def format_path(p: str):
        fname = os.path.basename(p)
        # í•­ìƒ data/â€¦ ë¡œ ì‹œì‘í•˜ë„ë¡ í•˜ë ¤ë©´ out_dir ì¸ìê°€ "data" ì—¬ì•¼ í•©ë‹ˆë‹¤.
        cur_dir = os.getcwd().split("\\")[-1]
        return f"{cur_dir}/{out_dir}/{split}/images/{fname}"

    df[url_col] = df['local_path'].apply(format_path)
    df.drop(columns=['local_path'], inplace=True)

    # 5) ê²°ê³¼ ì €ì¥
    os.makedirs(os.path.dirname(output_csv), exist_ok=True)
    df.to_csv(output_csv, index=False)
    print(f"âœ… [{Path(input_csv).name}] â†’ {output_csv} ({len(df)} rows)")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="raw_dir/*.csv â†’ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ + CSV ê°±ì‹ "
    )
    parser.add_argument("--splits", nargs="*",
                        help="ì²˜ë¦¬í•  split(ì˜ˆ: train test). ìƒëµ ì‹œ raw_dir/*.csv ì „ë¶€")
    parser.add_argument("--raw_dir", default="raw/QuIC360",
                        help="ì›ë³¸ CSV ë””ë ‰í† ë¦¬")
    parser.add_argument("--out_dir", default="quic360",
                        help="ì¶œë ¥ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ (ë§¨ ì•ì— ë¶™ì¼ í´ë”ëª…)")
    parser.add_argument("--url_col", default="url", help="URL ì»¬ëŸ¼ëª…")
    parser.add_argument("--max_concurrent", type=int, default=8, help="ë™ì‹œ ë‹¤ìš´ë¡œë“œ ìˆ˜")
    parser.add_argument("--requests_per_second", type=float, default=0.7, help="ì´ˆë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ")
    args = parser.parse_args()

    raw_dir = Path(args.raw_dir)
    out_dir = args.out_dir  # ì—¬ê¸°ì„œëŠ” ë¬¸ìì—´ "quic360" ê°€ ë˜ê¸¸ ê¸°ëŒ€

    if args.splits:
        splits = args.splits
    else:
        splits = [p.stem for p in raw_dir.glob("*.csv")]

    for split in splits:
        print(f"ğŸ”„ [{split}] ì²˜ë¦¬ ì¤‘â€¦")
        in_csv  = raw_dir / f"{split}.csv"
        out_csv = Path(out_dir) / f"{split}.csv"
        img_dir = Path(out_dir) / split / "images"

        if not in_csv.exists():
            print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {in_csv}, ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        process_split(
            input_csv=str(in_csv),
            output_csv=str(out_csv),
            image_dir=str(img_dir),
            url_col=args.url_col,
            max_concurrent=args.max_concurrent,
            rps=args.requests_per_second,
            out_dir=out_dir,
            split=split
        )