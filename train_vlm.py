import os
import argparse
import torch
import json
import gc
from transformers import (
    Trainer,
    TrainingArguments,
    default_data_collator,
    AutoProcessor,
)
from copy import deepcopy
from peft import LoraConfig, TaskType, get_peft_model
from transformers import TrainerCallback, AutoTokenizer
import yaml
from omegaconf import OmegaConf
from types import SimpleNamespace

from src.dataset import VLMDataset
from src.models.config import VisionLanguageConfig
from src.models.build import CustomVLMModel
from src.models.captionvlm import CaptioningVLM
from src.models.constant import (
    IGNORE_INDEX,
    IMAGE_TOKEN_INDEX,
    DEFAULT_IMAGE_TOKEN,
    DEFAULT_IMAGE_PATCH_TOKEN,
    DEFAULT_IM_START_TOKEN,
    DEFAULT_IM_END_TOKEN,
)

# ---------------------------------------------------------------------------- #
# GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
# ---------------------------------------------------------------------------- #
def print_gpu_memory_info(prefix=""):
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        print(f"{prefix} GPU ë©”ëª¨ë¦¬: í• ë‹¹={allocated:.2f}GB, ìºì‹œ={cached:.2f}GB")
    
def clear_gpu_cache():
    """GPU ìºì‹œ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

def estimate_memory_requirements(batch_size, sequence_length, hidden_size):
    """ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ì¶”ì • (ëŒ€ëµì )"""
    # ê°„ë‹¨í•œ ì¶”ì •ì‹ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•¨)
    estimated_gb = (batch_size * sequence_length * hidden_size * 4) / (1024**3)  # float32 ê¸°ì¤€
    return estimated_gb

def safe_json_serialize(obj):
    """JSON ì§ë ¬í™”ê°€ ì•ˆì „í•œ ê°ì²´ë¡œ ë³€í™˜"""
    if isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    elif isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, (list, tuple)):
        return [safe_json_serialize(item) for item in obj]
    elif isinstance(obj, dict):
        return {key: safe_json_serialize(value) for key, value in obj.items()}
    elif hasattr(obj, 'name'):  # Enum íƒ€ì…
        return obj.name
    elif hasattr(obj, '__dict__'):
        return str(obj)
    else:
        return str(obj)

# ---------------------------------------------------------------------------- #
# 0. Prompt Formatter
# ---------------------------------------------------------------------------- #
def create_input_with_template(instruction, tokenizer, image_placeholder=DEFAULT_IMAGE_TOKEN):
    """
    ì£¼ì–´ì§„ instructionê³¼ í…œí”Œë¦¿ì„ ê²°í•©í•œ í›„ í† í°í™”ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.

    Args:
    - instruction (str): ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§€ì‹œë¬¸.
    - image_placeholder (str): <image>ë¡œ ëŒ€ì²´ë  ì´ë¯¸ì§€ í† í°. ê¸°ë³¸ê°’ì€ "<image>".

    Returns:
    - dict: í† í¬ë‚˜ì´ì €ì— ì˜í•´ ì¸ì½”ë”©ëœ ì…ë ¥.
    """
    # í…œí”Œë¦¿ì„ ì •ì˜ (ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ ì˜ˆì‹œ)
    template = f"Here is an image prompt: {image_placeholder} Now answer the following question: {instruction}"

    # í…œí”Œë¦¿ê³¼ instructionì„ ê²°í•©í•˜ì—¬ í† í°í™”
    inputs = tokenizer(template, return_tensors="pt")

    return inputs

class LoRACheckpointCallback(TrainerCallback):
    """LoRA ê°€ì¤‘ì¹˜ì™€ ê´€ë ¨ êµ¬ì„±ìš”ì†Œë¥¼ í¬í•¨í•œ ì™„ì „í•œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì½œë°±"""
    
    def __init__(self, vision_processor, tokenizer, model_config, lora_config):
        self.vision_processor = vision_processor
        self.tokenizer = tokenizer
        self.model_config = model_config
        self.lora_config = lora_config

    def on_save(self, args, state, control, model=None, **kwargs):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ"""
        ckpt_dir = os.path.join(args.output_dir, f"checkpoint-{state.global_step}")
        
        print(f"\n=== LoRA ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œì‘: {ckpt_dir} ===")
        print_gpu_memory_info("ì €ì¥ ì „")
        
        # 1. LoRA ì–´ëŒ‘í„° ì €ì¥
        print("1. LoRA ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ì €ì¥...")
        model.save_pretrained(ckpt_dir)
        
        # 2. í† í¬ë‚˜ì´ì € ì €ì¥ (íŠ¹ìˆ˜ í† í° í¬í•¨)
        print("2. í† í¬ë‚˜ì´ì € ì €ì¥ (íŠ¹ìˆ˜ í† í° í¬í•¨)...")
        self.tokenizer.save_pretrained(ckpt_dir)
        
        # 3. ë¹„ì „ í”„ë¡œì„¸ì„œ ì €ì¥
        print("3. ë¹„ì „ í”„ë¡œì„¸ì„œ ì €ì¥...")
        self.vision_processor.save_pretrained(ckpt_dir)
        
        # 4. ëª¨ë¸ ì„¤ì • ì €ì¥
        print("4. VLM ëª¨ë¸ ì„¤ì • ì €ì¥...")
        model_config_path = os.path.join(ckpt_dir, "vlm_config.json")
        with open(model_config_path, 'w') as f:
            json.dump(self.model_config.to_dict(), f, indent=2, ensure_ascii=False)
        
        # 5. LoRA ì„¤ì • ì €ì¥ (JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ)
        print("5. LoRA ì„¤ì • ì €ì¥...")
        try:
            lora_config_dict = safe_json_serialize(self.lora_config.__dict__)
            lora_config_path = os.path.join(ckpt_dir, "lora_config.json")
            with open(lora_config_path, 'w') as f:
                json.dump(lora_config_dict, f, indent=2, ensure_ascii=False)
            print(f"   âœ“ LoRA ì„¤ì • ì €ì¥ ì™„ë£Œ: {len(lora_config_dict)} ê°œ í•­ëª©")
        except Exception as e:
            print(f"   âŒ LoRA ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
            # PEFTì˜ ê¸°ë³¸ ì„¤ì • íŒŒì¼ì´ ì´ë¯¸ ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
        
        # 6. ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ì €ì¥
        print("6. ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ì €ì¥...")
        try:
            metadata = {
                "step": state.global_step,
                "epoch": state.epoch if hasattr(state, 'epoch') else 0.0,
                "model_type": "CaptioningVLM_with_LoRA",
                "base_model": self.model_config.language_model_name,
                "vision_model": self.model_config.vision_model_name,
                "lora_r": self.lora_config.r,
                "lora_alpha": self.lora_config.lora_alpha,
                "target_modules": safe_json_serialize(self.lora_config.target_modules),
                "save_timestamp": f"step_{state.global_step}_epoch_{state.epoch:.2f}" if hasattr(state, 'epoch') else f"step_{state.global_step}",
                "training_loss": getattr(state, 'log_history', [])[-1].get('train_loss', 0.0) if hasattr(state, 'log_history') and state.log_history else 0.0
            }
            
            # ì „ì²´ ë©”íƒ€ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ì§ë ¬í™”
            safe_metadata = safe_json_serialize(metadata)
            
            metadata_path = os.path.join(ckpt_dir, "checkpoint_metadata.json")
            with open(metadata_path, 'w') as f:
                json.dump(safe_metadata, f, indent=2, ensure_ascii=False)
            print(f"   âœ“ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: step {state.global_step}")
        except Exception as e:
            print(f"   âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ë¼ë„ ì €ì¥ ì‹œë„
            basic_metadata = {
                "step": int(state.global_step),
                "epoch": float(state.epoch) if hasattr(state, 'epoch') else 0.0,
                "model_type": "CaptioningVLM_with_LoRA"
            }
            try:
                metadata_path = os.path.join(ckpt_dir, "checkpoint_metadata.json")
                with open(metadata_path, 'w') as f:
                    json.dump(basic_metadata, f, indent=2, ensure_ascii=False)
                print("   âœ“ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            except Exception as e2:
                print(f"   âŒ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì €ì¥ë„ ì‹¤íŒ¨: {e2}")
        
        # 7. ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        print("7. ë¡œë“œ ë„ìš°ë¯¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±...")
        self._create_load_script(ckpt_dir)
        
        # 8. ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_gpu_cache()
        print_gpu_memory_info("ì €ì¥ í›„")
        
        print(f"=== LoRA ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {ckpt_dir} ===\n")
    
    def _create_load_script(self, ckpt_dir):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œë¥¼ ìœ„í•œ ë„ìš°ë¯¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        load_script = f'''
# LoRA ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸
import torch
import os
from transformers import AutoTokenizer, AutoProcessor
from peft import PeftModel, LoraConfig, TaskType
import json
from src.models.config import VisionLanguageConfig
from src.models.captionvlm import CaptioningVLM

def safe_load_config(config_dict):
    """ì•ˆì „í•˜ê²Œ LoRA ì„¤ì •ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    # task_type ì²˜ë¦¬
    if isinstance(config_dict.get('task_type'), str):
        try:
            config_dict['task_type'] = getattr(TaskType, config_dict['task_type'])
        except AttributeError:
            config_dict['task_type'] = TaskType.CAUSAL_LM
    
    # target_modules ì²˜ë¦¬
    target_modules = config_dict.get('target_modules', [])
    if isinstance(target_modules, str):
        try:
            config_dict['target_modules'] = eval(target_modules)
        except:
            config_dict['target_modules'] = ["q_proj", "k_proj", "v_proj", "o_proj"]
    
    return config_dict

def load_lora_checkpoint(checkpoint_path="{ckpt_dir}"):
    """LoRA ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    
    try:
        # 1. ì„¤ì • íŒŒì¼ ë¡œë“œ
        with open(os.path.join(checkpoint_path, "vlm_config.json"), 'r') as f:
            model_config_dict = json.load(f)
        
        with open(os.path.join(checkpoint_path, "lora_config.json"), 'r') as f:
            lora_config_dict = json.load(f)
        
        # 2. í† í¬ë‚˜ì´ì € ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
        vision_processor = AutoProcessor.from_pretrained(checkpoint_path)
        
        # 3. VLM ì„¤ì • ì¬êµ¬ì„±
        model_config = VisionLanguageConfig(**model_config_dict)
        
        # 4. LoRA ì„¤ì • ì¬êµ¬ì„± (ì•ˆì „í•œ íƒ€ì… ë³€í™˜)
        safe_lora_config = safe_load_config(lora_config_dict)
        lora_config = LoraConfig(**safe_lora_config)
        
        # 5. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        base_model = CaptioningVLM(config=model_config, tokenizer=tokenizer)
        
        # 6. LoRA ì–´ëŒ‘í„° ì ìš©
        model = PeftModel.from_pretrained(base_model, checkpoint_path)
        
        return model, tokenizer, vision_processor
        
    except Exception as e:
        print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {{e}}")
        print("PEFT adapter_config.jsonì„ ì‚¬ìš©í•œ ê¸°ë³¸ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
        
        # ê¸°ë³¸ ë¡œë“œ ë°©ì‹
        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
        vision_processor = AutoProcessor.from_pretrained(checkpoint_path)
        
        # adapter_config.jsonì—ì„œ base_model_name_or_path ê°€ì ¸ì˜¤ê¸°
        adapter_config_path = os.path.join(checkpoint_path, "adapter_config.json")
        if os.path.exists(adapter_config_path):
            with open(adapter_config_path, 'r') as f:
                adapter_config = json.load(f)
                base_model_name = adapter_config.get('base_model_name_or_path', 'Qwen/Qwen3-0.6B')
        else:
            base_model_name = 'Qwen/Qwen3-0.6B'
        
        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ VLM ìƒì„±
        model_config = VisionLanguageConfig(
            language_model_name=base_model_name,
            vision_model_name="google/siglip-so400m-patch14-384"
        )
        
        base_model = CaptioningVLM(config=model_config, tokenizer=tokenizer)
        model = PeftModel.from_pretrained(base_model, checkpoint_path)
        
        return model, tokenizer, vision_processor

def load_for_inference(checkpoint_path="{ckpt_dir}"):
    """ì¶”ë¡ ìš©ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜"""
    model, tokenizer, processor = load_lora_checkpoint(checkpoint_path)
    model.eval()
    return model, tokenizer, processor

# ì‚¬ìš© ì˜ˆì‹œ:
# model, tokenizer, processor = load_lora_checkpoint()
# model.eval()

# ë˜ëŠ” ì¶”ë¡ ìš©:
# model, tokenizer, processor = load_for_inference()
'''
        
        script_path = os.path.join(ckpt_dir, "load_checkpoint.py")
        with open(script_path, 'w') as f:
            f.write(load_script)

# ---------------------------------------------------------------------------- #
# 2. Collator
# ---------------------------------------------------------------------------- #
class MultimodalCollator:
    def __init__(self, tokenizer=None, pad_token_id=0):
        self.tokenizer = tokenizer
        # í† í¬ë‚˜ì´ì €ê°€ ì œê³µëœ ê²½ìš° í•´ë‹¹ pad_token_id ì‚¬ìš©
        self.pad_token_id = tokenizer.pad_token_id if tokenizer is not None else pad_token_id
    
    def __call__(self, features):
        # 1. ë¹„ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬ (pixel_values)
        # í”„ë ˆì„ ì°¨ì› êµ¬ì¡° í™•ì¸ ë° ë°°ì¹˜ êµ¬ì„±
        frames, c, h, w = features[0]['pixel_values'].shape
        pixel_values = torch.stack([f['pixel_values'] for f in features])
        
        # 2. í…ìŠ¤íŠ¸ ë°ì´í„° íŒ¨ë”© ë° ì²˜ë¦¬
        # ìµœëŒ€ ê¸¸ì´ ê³„ì‚°
        max_length = max(f['input_ids'].size(0) for f in features)
        
        # íŒ¨ë”©ëœ ë°°ì¹˜ í…ì„œ ì´ˆê¸°í™”
        batch_size = len(features)
        input_ids = torch.full((batch_size, max_length), 
                               self.pad_token_id, 
                               dtype=features[0]['input_ids'].dtype,
                               device=features[0]['input_ids'].device)
        attention_mask = torch.zeros((batch_size, max_length), 
                                    dtype=features[0]['attention_mask'].dtype,
                                    device=features[0]['attention_mask'].device)
        
        # ê° ìƒ˜í”Œì— ëŒ€í•´ íŒ¨ë”© ì ìš©
        for i, f in enumerate(features):
            input_len = f['input_ids'].size(0)
            input_ids[i, :input_len] = f['input_ids']
            attention_mask[i, :input_len] = f['attention_mask']
        
        # ê¸°ë³¸ ë°°ì¹˜ êµ¬ì„±
        batch = {
            'pixel_values': pixel_values,
            'input_ids': input_ids,
            'attention_mask': attention_mask,
        }
        
        # 3. ë¼ë²¨ ì²˜ë¦¬ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
        if 'labels' in features[0]:
            # ê¸°ë³¸ì ìœ¼ë¡œ -100ìœ¼ë¡œ ì´ˆê¸°í™”ëœ ë¼ë²¨ í…ì„œ ìƒì„±
            labels = torch.full((batch_size, max_length), 
                               -100,  # IGNORE_INDEX
                               dtype=features[0]['labels'].dtype,
                               device=features[0]['labels'].device)
            
            # ê° ìƒ˜í”Œì˜ ë¼ë²¨ ë³µì‚¬
            for i, f in enumerate(features):
                label_len = f['labels'].size(0)
                labels[i, :label_len] = f['labels']
                
            # ì£¼ì˜: ë¼ë²¨ì€ ì´ë¯¸ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ë˜ì–´ ìˆìœ¼ë¯€ë¡œ 
            # ì¶”ê°€ íŒ¨ë”© ë§ˆìŠ¤í‚¹ì€ í•„ìš”í•˜ì§€ ì•ŠìŒ
            batch['labels'] = labels
        
        # 4. ë¹„í…ì„œ ë°ì´í„° ì²˜ë¦¬ (ê²½ë¡œ ë“±)
        if 'video_path' in features[0]:
            batch['video_path'] = [f['video_path'] for f in features]
        
        return batch

# ---------------------------------------------------------------------------- #
# 3. Argument Parser
# ---------------------------------------------------------------------------- #
def parse_args():
    parser = argparse.ArgumentParser(description="Train VLM with parameters from YAML")
    parser.add_argument("--config", type=str, default="config/train.yaml", help="Path to the YAML config")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None, 
                       help="Path to LoRA checkpoint to resume from")
    return parser.parse_args()

def load_config(config_path: str):
    """
    Load YAML into an OmegaConf DictConfig so we can keep dotâ€‘access,
    then resolve any {model_name} placeholders.
    """
    cfg = OmegaConf.load(config_path)

    # Resolve {model_name} templates
    model_name = cfg.model.name
    cfg.training.output_dir = cfg.training.output_dir.format(model_name=model_name)
    cfg.training.run_name   = cfg.training.run_name.format(model_name=model_name)
    cfg.training.logging_dir = cfg.training.logging_dir.format(model_name=model_name)
    # Backward compatibility: allow config with only "data:" section
    if "dataset" not in cfg and "data" in cfg:
        cfg.dataset = cfg.data  # backward compatibility alias
    return cfg

def load_lora_checkpoint(checkpoint_path, model_config, lora_config):
    """LoRA ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    print(f"LoRA ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ: {checkpoint_path}")
    
    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„±
        base_model = CaptioningVLM(config=model_config, tokenizer=tokenizer)
        
        # LoRA ì–´ëŒ‘í„° ì ìš©
        from peft import PeftModel
        model = PeftModel.from_pretrained(base_model, checkpoint_path)
        
        print("LoRA ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
        return model, tokenizer
        
    except Exception as e:
        print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ìƒˆ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œë„
        try:
            tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
        except:
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëª¨ë¸ì—ì„œ ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(model_config.language_model_name)
            
            # í† í¬ë‚˜ì´ì € ì„¤ì •
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "right"
            
            # íŠ¹ìˆ˜ í† í° ì¶”ê°€
            special_tokens = {"additional_special_tokens": [
                DEFAULT_IMAGE_TOKEN,
                DEFAULT_IMAGE_PATCH_TOKEN, 
                DEFAULT_IM_START_TOKEN,
                DEFAULT_IM_END_TOKEN,
            ]}
            tokenizer.add_special_tokens(special_tokens)
        
        # ìƒˆ ëª¨ë¸ ìƒì„±
        base_model = CaptioningVLM(config=model_config, tokenizer=tokenizer)
        from peft import get_peft_model
        model = get_peft_model(base_model, lora_config)
        
        return model, tokenizer

# ---------------------------------------------------------------------------- #
# 4. Main training flow
# ---------------------------------------------------------------------------- #
def main():
    args = parse_args()
    cfg = load_config(args.config)
    
    print("=== í›ˆë ¨ ì„¤ì • ì •ë³´ ===")
    print(f"ë°°ì¹˜ í¬ê¸°: {cfg.training.batch_size.train}")
    print(f"Gradient Accumulation Steps: {cfg.training.gradient_accumulation_steps}")
    print(f"ì‹¤ì œ ë°°ì¹˜ í¬ê¸°: {cfg.training.batch_size.train * cfg.training.gradient_accumulation_steps}")
    print_gpu_memory_info("ì´ˆê¸°")
    
    # Config ì„¤ì •
    model_config = VisionLanguageConfig(
        vision_model_name=cfg.model.vision_model_name,
        language_model_name=cfg.model.llm_model_name,
        projector_type=cfg.model.projector_type,
        use_resampler=cfg.model.use_resampler,
        mm_spatial_pool_mode=cfg.model.mm_spatial_pool_mode,
        mm_newline_position=getattr(cfg.model, "mm_newline_position", "grid"),
        freeze_vision=cfg.model.freeze_vision,
        freeze_llm=cfg.model.freeze_llm,
    )

    vision_processor = AutoProcessor.from_pretrained(cfg.model.vision_model_name)
    print_gpu_memory_info("í”„ë¡œì„¸ì„œ ë¡œë“œ í›„")
    
    # LoRA ì„¤ì •
    lora_config = LoraConfig(
        task_type       = TaskType.CAUSAL_LM,
        r               = 128,
        lora_alpha      = 64,
        lora_dropout    = 0.1,
        base_model_name_or_path = cfg.model.llm_model_name,
        target_modules  = [
            "q_proj", "k_proj", "v_proj",          # attention
            "o_proj",
            "gate_proj", "up_proj", "down_proj"    # MLP
        ],
        bias            = "none"
    )
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œí•˜ëŠ” ê²½ìš°ì™€ ìƒˆë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° êµ¬ë¶„
    if args.resume_from_checkpoint and os.path.exists(args.resume_from_checkpoint):
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ
        model, language_processor = load_lora_checkpoint(
            args.resume_from_checkpoint, model_config, lora_config
        )
        print(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ í›ˆë ¨ ì¬ê°œ: {args.resume_from_checkpoint}")
    else:
        # ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±
        language_processor = AutoTokenizer.from_pretrained(cfg.model.llm_model_name)

        # í† í¬ë‚˜ì´ì € ì„¤ì •
        if language_processor.pad_token is None:
            language_processor.pad_token = language_processor.eos_token
        language_processor.padding_side = "right"

        # íŠ¹ìˆ˜ í† í° ì¶”ê°€
        special_tokens = {"additional_special_tokens": [
            DEFAULT_IMAGE_TOKEN,
            DEFAULT_IMAGE_PATCH_TOKEN, 
            DEFAULT_IM_START_TOKEN,
            DEFAULT_IM_END_TOKEN,
        ]}
        language_processor.add_special_tokens(special_tokens)

        print_gpu_memory_info("í† í¬ë‚˜ì´ì € ì„¤ì • í›„")

        # ëª¨ë¸ ì´ˆê¸°í™”
        model = CaptioningVLM(
            config=model_config, 
            tokenizer=language_processor
        )
        print_gpu_memory_info("ëª¨ë¸ ì´ˆê¸°í™” í›„")

        # LoRA ì ìš©
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        print("ìƒˆë¡œìš´ LoRA ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        print_gpu_memory_info("LoRA ì ìš© í›„")

    # Dataset ì„¤ì •
    ds_cfg = cfg.dataset
    train_ds = VLMDataset(
        data_path           = ds_cfg.data_path,
        data_files          = getattr(ds_cfg, "data_files", getattr(ds_cfg, "train_file", None)),
        image_placeholder   = DEFAULT_IMAGE_TOKEN,
        max_frames_num      = getattr(ds_cfg, "max_frames_num", 64),
        fps                 = getattr(ds_cfg, "fps", 1),
        img_processor       = vision_processor,
        tokenizer           = language_processor,
        force_sample        = getattr(ds_cfg, "force_sample", False),
    )
    print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(train_ds)}")

    # GPUë¡œ ì´ë™ ë° ë©”ëª¨ë¦¬ ìµœì í™”
    if torch.cuda.is_available():
        model.cuda()
        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ gradient checkpointing í™œì„±í™”
        model.gradient_checkpointing_enable()
        print(f"ëª¨ë¸ì„ GPUë¡œ ì´ë™, Gradient Checkpointing í™œì„±í™”")
        print_gpu_memory_info("GPU ì´ë™ í›„")
        
        # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ê¶Œì¥ì‚¬í•­ ì¶œë ¥
        allocated_gb = torch.cuda.memory_allocated() / 1024**3
        if allocated_gb > 8:  # 8GB ì´ìƒ ì‚¬ìš© ì‹œ ê²½ê³ 
            print("\nâš ï¸  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì„¤ì •ì„ ê³ ë ¤í•´ë³´ì„¸ìš”:")
            print(f"   - ë°°ì¹˜ í¬ê¸°ë¥¼ {cfg.training.batch_size.train // 2}ë¡œ ì¤„ì´ê¸°")
            print(f"   - Gradient accumulation stepsë¥¼ {cfg.training.gradient_accumulation_steps * 2}ë¡œ ëŠ˜ë¦¬ê¸°")
            print("   - max_frames_numì„ 32ë¡œ ì¤„ì´ê¸°")
            print("   - DeepSpeed ZeRO ì‚¬ìš©í•˜ê¸°")
    else:
        print("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUì—ì„œ í›ˆë ¨ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # í›ˆë ¨ ì„¤ì • - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
    training_args = TrainingArguments(
        output_dir=cfg.training.output_dir,
        run_name=cfg.training.run_name,
        logging_dir=cfg.training.logging_dir,
        deepspeed=cfg.deepspeed.config if "deepspeed" in cfg and cfg.deepspeed.enabled else None,

        num_train_epochs=cfg.training.num_train_epochs,
        per_device_train_batch_size=cfg.training.batch_size.train,
        
        gradient_accumulation_steps=cfg.training.gradient_accumulation_steps,
        gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í™œì„±í™”
        learning_rate=cfg.training.learning_rate,
        weight_decay=cfg.training.weight_decay,
        warmup_ratio=cfg.training.warmup_ratio,
        dataloader_num_workers=cfg.training.dataloader_num_workers,
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„  ì˜µì…˜ë“¤
        fp16=True,  # Mixed precision training í™œì„±í™”
        dataloader_pin_memory=False,  # ë©”ëª¨ë¦¬ ì ˆì•½
        remove_unused_columns=False,  # í•„ìš”í•œ ì»¬ëŸ¼ ë³´ì¡´
        
        # ì²´í¬í¬ì¸íŠ¸ ë° ë¡œê¹… ì„¤ì •
        save_strategy=cfg.training.save_strategy,
        save_steps=cfg.training.save_steps,
        save_total_limit=cfg.training.save_total_limit,
        load_best_model_at_end=False,

        report_to=cfg.training.report_to,
        logging_steps=cfg.training.logging_steps,
        max_grad_norm=cfg.training.max_grad_norm,
        
        # ë ˆì´ë¸” ê´€ë ¨ ì„¤ì •
        label_names=["labels"],  # PeftModel ê²½ê³  í•´ê²°
    )
    
    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    data_collator = MultimodalCollator()
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        data_collator=data_collator,
        processing_class=language_processor,  # tokenizer ëŒ€ì‹  processing_class ì‚¬ìš©
    )
    
    # LoRA ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì¶”ê°€
    lora_callback = LoRACheckpointCallback(
        vision_processor=vision_processor,
        tokenizer=language_processor,
        model_config=model_config,
        lora_config=lora_config
    )
    trainer.add_callback(lora_callback)
    
    # í›ˆë ¨ ì‹œì‘
    try:
        if args.resume_from_checkpoint:
            trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
        else:
            trainer.train()
    except torch.cuda.OutOfMemoryError as e:
        print(f"\nâŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nğŸ”§ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ê¶Œì¥ ì„¤ì •:")
        print("1. config/train.yamlì—ì„œ ë‹¤ìŒ ê°’ë“¤ì„ ì¤„ì—¬ë³´ì„¸ìš”:")
        print(f"   - batch_size.train: {cfg.training.batch_size.train} â†’ {max(1, cfg.training.batch_size.train // 2)}")
        print(f"   - max_frames_num: {getattr(cfg.dataset, 'max_frames_num', 64)} â†’ 32")
        print("2. gradient_accumulation_stepsë¥¼ ëŠ˜ë ¤ì„œ effective batch size ìœ ì§€")
        print("3. DeepSpeed ZeRO Stage 2 í™œì„±í™”")
        print("4. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© ê³ ë ¤")
        raise
    
    # ìµœì¢… ë³‘í•© ëª¨ë¸ ì €ì¥
    print("\n=== ìµœì¢… ë³‘í•© ëª¨ë¸ ì €ì¥ ì‹œì‘ ===")
    print_gpu_memory_info("ë³‘í•© ì „")
    
    try:
        merged_model = model.merge_and_unload()
        print_gpu_memory_info("ë³‘í•© í›„")
        
        save_dir = os.path.join(training_args.output_dir, "merged_final")
        os.makedirs(save_dir, exist_ok=True)
        
        merged_model.save_pretrained(save_dir, safe_serialization=True)
        language_processor.save_pretrained(save_dir)
        vision_processor.save_pretrained(save_dir)
        
        # ìµœì¢… ëª¨ë¸ ì„¤ì •ë„ ì €ì¥
        final_config_path = os.path.join(save_dir, "vlm_config.json")
        with open(final_config_path, 'w') as f:
            json.dump(model_config.to_dict(), f, indent=2, ensure_ascii=False)
        
        clear_gpu_cache()
        print(f"=== ìµœì¢… ë³‘í•© ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_dir} ===")
        print_gpu_memory_info("ìµœì¢…")
        
    except Exception as e:
        print(f"âŒ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        raise

# ---------------------------------------------------------------------------- #
if __name__ == "__main__":
    main()